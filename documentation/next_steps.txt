Learning Scheduler: linear, cosine
Learning Rate: [5e-5]
Beta: 0.1
Warmup Ratio: 0.1/0.05/
Weight Decay:


Keep GAS fixed


Next Steps:
DONE: ensure no post is there in more than one split
perplexity
sample evaluation at epoch level
logging to csv example level scores [to compare accuracies] [example level comparison of failure]

Do we need to try out llama2 7b chat?

Changes for Implementing perplexity:
1. concatenated_forward: DONE
    * calculate perplexity using logits/logps and the API creating function calculate_perplexity
2. Modified functions that call concatenated_forward to receive perplexity scores
    - compute_reference_log_probs: DONE
        * received ppl values and returned them

        Check dpo_trainer -> get_batch_loss_metrics -> if condition:
        Because there is an option to precompute reference using the flag: precompute_ref_log_probs, we make changes to the below ones too 
        - get_eval_dataloader: DONE

        - get_train_dataloader: DONE

    - get_batch_loss_metrics
        - changed aux_loss handling
Changes for Implementing Sample Evaluation at Epoch level; DONE

Changes for Implementing Logging to csv:
1. Log prompt, chosen, 
